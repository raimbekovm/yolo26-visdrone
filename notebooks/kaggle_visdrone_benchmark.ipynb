{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO26 vs YOLO11 Benchmark on VisDrone Dataset\n",
    "\n",
    "This notebook provides a comprehensive benchmark comparing **YOLO26** and **YOLO11** on the VisDrone dataset for small object detection.\n",
    "\n",
    "## Goals\n",
    "1. Verify **43% faster CPU inference** claim for YOLO26\n",
    "2. Compare **small object detection** performance (ProgLoss + STAL features)\n",
    "3. Evaluate **NMS-free end-to-end inference** benefits\n",
    "\n",
    "## Author\n",
    "**Murat Raimbekov** - [GitHub](https://github.com/raimbekovm) | [HuggingFace](https://huggingface.co/raimbekovm)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q ultralytics>=8.3.0 pycocotools>=2.0.7 huggingface_hub pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check environment\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"epochs\": 100,\n",
    "    \"batch\": 16,\n",
    "    \"imgsz\": 640,\n",
    "    \"device\": 0 if torch.cuda.is_available() else \"cpu\",\n",
    "    \"project\": \"runs/detect\",\n",
    "    \"data\": \"VisDrone.yaml\",\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download VisDrone Dataset\n",
    "\n",
    "VisDrone is automatically downloaded by Ultralytics when training with `VisDrone.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset download (will download if not exists)\n",
    "# First train call with epochs=0 will trigger download\n",
    "print(\"Checking VisDrone dataset...\")\n",
    "print(\"Dataset will be downloaded automatically during first training.\")\n",
    "\n",
    "# VisDrone classes\n",
    "VISDRONE_CLASSES = [\n",
    "    \"pedestrian\", \"people\", \"bicycle\", \"car\", \"van\",\n",
    "    \"truck\", \"tricycle\", \"awning-tricycle\", \"bus\", \"motor\"\n",
    "]\n",
    "\n",
    "print(f\"\\nVisDrone Classes ({len(VISDRONE_CLASSES)}):\")\n",
    "for i, cls in enumerate(VISDRONE_CLASSES):\n",
    "    print(f\"  {i}: {cls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train YOLO26n on VisDrone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize YOLO26n model\n",
    "yolo26 = YOLO(\"yolo26n.pt\")\n",
    "\n",
    "# Print model info\n",
    "print(\"YOLO26n Model Info:\")\n",
    "yolo26.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO26n\n",
    "print(\"=\"*60)\n",
    "print(\"Training YOLO26n on VisDrone\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "yolo26_results = yolo26.train(\n",
    "    data=CONFIG[\"data\"],\n",
    "    epochs=CONFIG[\"epochs\"],\n",
    "    batch=CONFIG[\"batch\"],\n",
    "    imgsz=CONFIG[\"imgsz\"],\n",
    "    device=CONFIG[\"device\"],\n",
    "    project=CONFIG[\"project\"],\n",
    "    name=\"yolo26n_visdrone\",\n",
    "    exist_ok=True,\n",
    "    plots=True,\n",
    "    save=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save YOLO26n path\n",
    "YOLO26_WEIGHTS = f\"{CONFIG['project']}/yolo26n_visdrone/weights/best.pt\"\n",
    "print(f\"YOLO26n best weights: {YOLO26_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train YOLO11n on VisDrone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize YOLO11n model\n",
    "yolo11 = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Print model info\n",
    "print(\"YOLO11n Model Info:\")\n",
    "yolo11.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO11n\n",
    "print(\"=\"*60)\n",
    "print(\"Training YOLO11n on VisDrone\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "yolo11_results = yolo11.train(\n",
    "    data=CONFIG[\"data\"],\n",
    "    epochs=CONFIG[\"epochs\"],\n",
    "    batch=CONFIG[\"batch\"],\n",
    "    imgsz=CONFIG[\"imgsz\"],\n",
    "    device=CONFIG[\"device\"],\n",
    "    project=CONFIG[\"project\"],\n",
    "    name=\"yolo11n_visdrone\",\n",
    "    exist_ok=True,\n",
    "    plots=True,\n",
    "    save=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save YOLO11n path\n",
    "YOLO11_WEIGHTS = f\"{CONFIG['project']}/yolo11n_visdrone/weights/best.pt\"\n",
    "print(f\"YOLO11n best weights: {YOLO11_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation & Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "yolo26_trained = YOLO(YOLO26_WEIGHTS)\n",
    "yolo11_trained = YOLO(YOLO11_WEIGHTS)\n",
    "\n",
    "print(\"Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate YOLO26n\n",
    "print(\"\\nValidating YOLO26n...\")\n",
    "yolo26_val = yolo26_trained.val(\n",
    "    data=CONFIG[\"data\"],\n",
    "    imgsz=CONFIG[\"imgsz\"],\n",
    "    device=CONFIG[\"device\"],\n",
    ")\n",
    "\n",
    "# Validate YOLO11n\n",
    "print(\"\\nValidating YOLO11n...\")\n",
    "yolo11_val = yolo11_trained.val(\n",
    "    data=CONFIG[\"data\"],\n",
    "    imgsz=CONFIG[\"imgsz\"],\n",
    "    device=CONFIG[\"device\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics\n",
    "metrics_data = {\n",
    "    \"Model\": [\"YOLO26n\", \"YOLO11n\"],\n",
    "    \"mAP50\": [yolo26_val.box.map50, yolo11_val.box.map50],\n",
    "    \"mAP50-95\": [yolo26_val.box.map, yolo11_val.box.map],\n",
    "    \"Precision\": [yolo26_val.box.mp, yolo11_val.box.mp],\n",
    "    \"Recall\": [yolo26_val.box.mr, yolo11_val.box.mr],\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Overall Metrics Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COCO Evaluation - mAP by Object Size\n",
    "\n",
    "Using pycocotools to get mAP breakdown by object size (small, medium, large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from PIL import Image\n",
    "\n",
    "def create_coco_annotations(data_path, split=\"val\"):\n",
    "    \"\"\"Create COCO-format annotations from VisDrone labels.\"\"\"\n",
    "    split_map = {\n",
    "        \"train\": \"VisDrone2019-DET-train\",\n",
    "        \"val\": \"VisDrone2019-DET-val\",\n",
    "        \"test\": \"VisDrone2019-DET-test-dev\",\n",
    "    }\n",
    "    \n",
    "    split_dir = Path(data_path) / split_map[split]\n",
    "    images_dir = split_dir / \"images\"\n",
    "    labels_dir = split_dir / \"labels\"\n",
    "    \n",
    "    coco_dict = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [\n",
    "            {\"id\": i, \"name\": name}\n",
    "            for i, name in enumerate(VISDRONE_CLASSES)\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    annotation_id = 0\n",
    "    image_files = sorted(list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\")))\n",
    "    \n",
    "    for img_id, img_path in enumerate(image_files):\n",
    "        with Image.open(img_path) as img:\n",
    "            width, height = img.size\n",
    "        \n",
    "        coco_dict[\"images\"].append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": img_path.name,\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "        })\n",
    "        \n",
    "        label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
    "        if label_path.exists():\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        cls_id = int(parts[0])\n",
    "                        x_center = float(parts[1]) * width\n",
    "                        y_center = float(parts[2]) * height\n",
    "                        box_width = float(parts[3]) * width\n",
    "                        box_height = float(parts[4]) * height\n",
    "                        \n",
    "                        x_min = x_center - box_width / 2\n",
    "                        y_min = y_center - box_height / 2\n",
    "                        \n",
    "                        coco_dict[\"annotations\"].append({\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": img_id,\n",
    "                            \"category_id\": cls_id,\n",
    "                            \"bbox\": [x_min, y_min, box_width, box_height],\n",
    "                            \"area\": box_width * box_height,\n",
    "                            \"iscrowd\": 0,\n",
    "                        })\n",
    "                        annotation_id += 1\n",
    "    \n",
    "    return coco_dict\n",
    "\n",
    "def generate_predictions(model, data_path, split=\"val\"):\n",
    "    \"\"\"Generate COCO-format predictions.\"\"\"\n",
    "    split_map = {\n",
    "        \"train\": \"VisDrone2019-DET-train\",\n",
    "        \"val\": \"VisDrone2019-DET-val\",\n",
    "    }\n",
    "    \n",
    "    split_dir = Path(data_path) / split_map[split]\n",
    "    images_dir = split_dir / \"images\"\n",
    "    \n",
    "    predictions = []\n",
    "    image_files = sorted(list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\")))\n",
    "    \n",
    "    print(f\"Generating predictions for {len(image_files)} images...\")\n",
    "    \n",
    "    for img_id, img_path in enumerate(image_files):\n",
    "        results = model.predict(str(img_path), verbose=False)[0]\n",
    "        \n",
    "        if results.boxes is not None and len(results.boxes):\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            scores = results.boxes.conf.cpu().numpy()\n",
    "            classes = results.boxes.cls.cpu().numpy()\n",
    "            \n",
    "            for box, score, cls in zip(boxes, scores, classes):\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                width = x_max - x_min\n",
    "                height = y_max - y_min\n",
    "                \n",
    "                predictions.append({\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": int(cls),\n",
    "                    \"bbox\": [float(x_min), float(y_min), float(width), float(height)],\n",
    "                    \"score\": float(score),\n",
    "                })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"COCO evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find VisDrone dataset path\n",
    "# Usually in ~/datasets/VisDrone or ./datasets/VisDrone\n",
    "possible_paths = [\n",
    "    Path(\"datasets/VisDrone\"),\n",
    "    Path.home() / \"datasets\" / \"VisDrone\",\n",
    "    Path(\"/kaggle/input/visdrone\"),\n",
    "]\n",
    "\n",
    "VISDRONE_PATH = None\n",
    "for p in possible_paths:\n",
    "    if p.exists():\n",
    "        VISDRONE_PATH = p\n",
    "        break\n",
    "\n",
    "if VISDRONE_PATH:\n",
    "    print(f\"Found VisDrone dataset at: {VISDRONE_PATH}\")\n",
    "else:\n",
    "    print(\"VisDrone dataset not found. Please check paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run COCO evaluation if dataset found\n",
    "if VISDRONE_PATH:\n",
    "    print(\"Creating ground truth annotations...\")\n",
    "    gt_dict = create_coco_annotations(VISDRONE_PATH, \"val\")\n",
    "    \n",
    "    # Create COCO object\n",
    "    coco_gt = COCO()\n",
    "    coco_gt.dataset = gt_dict\n",
    "    coco_gt.createIndex()\n",
    "    \n",
    "    print(f\"Ground truth: {len(gt_dict['images'])} images, {len(gt_dict['annotations'])} annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate YOLO26\n",
    "if VISDRONE_PATH:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COCO Evaluation: YOLO26n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    yolo26_preds = generate_predictions(yolo26_trained, VISDRONE_PATH, \"val\")\n",
    "    coco_dt_26 = coco_gt.loadRes(yolo26_preds)\n",
    "    \n",
    "    coco_eval_26 = COCOeval(coco_gt, coco_dt_26, \"bbox\")\n",
    "    coco_eval_26.evaluate()\n",
    "    coco_eval_26.accumulate()\n",
    "    coco_eval_26.summarize()\n",
    "    \n",
    "    yolo26_stats = coco_eval_26.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate YOLO11\n",
    "if VISDRONE_PATH:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COCO Evaluation: YOLO11n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    yolo11_preds = generate_predictions(yolo11_trained, VISDRONE_PATH, \"val\")\n",
    "    coco_dt_11 = coco_gt.loadRes(yolo11_preds)\n",
    "    \n",
    "    coco_eval_11 = COCOeval(coco_gt, coco_dt_11, \"bbox\")\n",
    "    coco_eval_11.evaluate()\n",
    "    coco_eval_11.accumulate()\n",
    "    coco_eval_11.summarize()\n",
    "    \n",
    "    yolo11_stats = coco_eval_11.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare size-based metrics\n",
    "if VISDRONE_PATH:\n",
    "    size_data = {\n",
    "        \"Model\": [\"YOLO26n\", \"YOLO11n\"],\n",
    "        \"AP_overall\": [yolo26_stats[0], yolo11_stats[0]],\n",
    "        \"AP_small\": [yolo26_stats[3], yolo11_stats[3]],\n",
    "        \"AP_medium\": [yolo26_stats[4], yolo11_stats[4]],\n",
    "        \"AP_large\": [yolo26_stats[5], yolo11_stats[5]],\n",
    "    }\n",
    "    \n",
    "    size_df = pd.DataFrame(size_data)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AP by Object Size (COCO Evaluation)\")\n",
    "    print(\"=\"*60)\n",
    "    print(size_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate improvement\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"YOLO26n Improvement over YOLO11n:\")\n",
    "    for metric in [\"AP_small\", \"AP_medium\", \"AP_large\"]:\n",
    "        diff = size_df[size_df[\"Model\"]==\"YOLO26n\"][metric].values[0] - size_df[size_df[\"Model\"]==\"YOLO11n\"][metric].values[0]\n",
    "        sign = \"+\" if diff > 0 else \"\"\n",
    "        print(f\"  {metric}: {sign}{diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Speed Benchmark\n",
    "\n",
    "Comparing inference speed on GPU and CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_speed(model, device, warmup=10, runs=100, imgsz=640):\n",
    "    \"\"\"Benchmark inference speed.\"\"\"\n",
    "    # Create dummy input\n",
    "    dummy_input = np.random.randint(0, 255, (imgsz, imgsz, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Warmup\n",
    "    print(f\"  Warmup ({warmup} runs)...\")\n",
    "    for _ in range(warmup):\n",
    "        model.predict(dummy_input, device=device, verbose=False)\n",
    "    \n",
    "    # Synchronize GPU\n",
    "    if device != \"cpu\" and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    print(f\"  Benchmarking ({runs} runs)...\")\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        model.predict(dummy_input, device=device, verbose=False)\n",
    "        if device != \"cpu\" and torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000)  # ms\n",
    "    \n",
    "    times = np.array(times)\n",
    "    return {\n",
    "        \"mean_ms\": np.mean(times),\n",
    "        \"std_ms\": np.std(times),\n",
    "        \"min_ms\": np.min(times),\n",
    "        \"max_ms\": np.max(times),\n",
    "        \"fps\": 1000 / np.mean(times),\n",
    "    }\n",
    "\n",
    "print(\"Speed benchmark function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GPU benchmark\n",
    "speed_results = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GPU Speed Benchmark\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nYOLO26n on GPU:\")\n",
    "    yolo26_gpu = benchmark_speed(yolo26_trained, device=0)\n",
    "    speed_results.append({\"Model\": \"YOLO26n\", \"Device\": \"GPU\", **yolo26_gpu})\n",
    "    \n",
    "    print(\"\\nYOLO11n on GPU:\")\n",
    "    yolo11_gpu = benchmark_speed(yolo11_trained, device=0)\n",
    "    speed_results.append({\"Model\": \"YOLO11n\", \"Device\": \"GPU\", **yolo11_gpu})\n",
    "    \n",
    "    gpu_speedup = yolo11_gpu[\"mean_ms\"] / yolo26_gpu[\"mean_ms\"]\n",
    "    print(f\"\\nGPU Speedup: YOLO26 is {gpu_speedup:.2f}x faster than YOLO11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CPU benchmark\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CPU Speed Benchmark\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nYOLO26n on CPU:\")\n",
    "yolo26_cpu = benchmark_speed(yolo26_trained, device=\"cpu\")\n",
    "speed_results.append({\"Model\": \"YOLO26n\", \"Device\": \"CPU\", **yolo26_cpu})\n",
    "\n",
    "print(\"\\nYOLO11n on CPU:\")\n",
    "yolo11_cpu = benchmark_speed(yolo11_trained, device=\"cpu\")\n",
    "speed_results.append({\"Model\": \"YOLO11n\", \"Device\": \"CPU\", **yolo11_cpu})\n",
    "\n",
    "cpu_speedup = yolo11_cpu[\"mean_ms\"] / yolo26_cpu[\"mean_ms\"]\n",
    "print(f\"\\nCPU Speedup: YOLO26 is {cpu_speedup:.2f}x faster than YOLO11\")\n",
    "print(f\"Claimed speedup: 1.43x (43% faster)\")\n",
    "print(f\"Actual speedup: {cpu_speedup:.2f}x ({(cpu_speedup-1)*100:.1f}% faster)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display speed results\n",
    "speed_df = pd.DataFrame(speed_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Speed Benchmark Results\")\n",
    "print(\"=\"*60)\n",
    "print(speed_df[[\"Model\", \"Device\", \"mean_ms\", \"std_ms\", \"fps\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "colors = {'YOLO26n': '#2ecc71', 'YOLO11n': '#3498db'}\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. mAP Comparison\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, [metrics_df[metrics_df['Model']=='YOLO26n']['mAP50'].values[0], \n",
    "                      metrics_df[metrics_df['Model']=='YOLO26n']['mAP50-95'].values[0]], \n",
    "        width, label='YOLO26n', color=colors['YOLO26n'])\n",
    "ax1.bar(x + width/2, [metrics_df[metrics_df['Model']=='YOLO11n']['mAP50'].values[0], \n",
    "                      metrics_df[metrics_df['Model']=='YOLO11n']['mAP50-95'].values[0]], \n",
    "        width, label='YOLO11n', color=colors['YOLO11n'])\n",
    "ax1.set_ylabel('mAP')\n",
    "ax1.set_title('Overall mAP Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(['mAP50', 'mAP50-95'])\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# 2. AP by Size (if available)\n",
    "ax2 = axes[0, 1]\n",
    "if VISDRONE_PATH:\n",
    "    x = np.arange(3)\n",
    "    ax2.bar(x - width/2, [size_df[size_df['Model']=='YOLO26n']['AP_small'].values[0],\n",
    "                          size_df[size_df['Model']=='YOLO26n']['AP_medium'].values[0],\n",
    "                          size_df[size_df['Model']=='YOLO26n']['AP_large'].values[0]], \n",
    "            width, label='YOLO26n', color=colors['YOLO26n'])\n",
    "    ax2.bar(x + width/2, [size_df[size_df['Model']=='YOLO11n']['AP_small'].values[0],\n",
    "                          size_df[size_df['Model']=='YOLO11n']['AP_medium'].values[0],\n",
    "                          size_df[size_df['Model']=='YOLO11n']['AP_large'].values[0]], \n",
    "            width, label='YOLO11n', color=colors['YOLO11n'])\n",
    "    ax2.set_ylabel('AP')\n",
    "    ax2.set_title('AP by Object Size (COCO Evaluation)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(['Small (<32px)', 'Medium (32-96px)', 'Large (>96px)'])\n",
    "    ax2.legend()\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'COCO evaluation not available', ha='center', va='center')\n",
    "    ax2.set_title('AP by Object Size')\n",
    "\n",
    "# 3. CPU Speed Comparison\n",
    "ax3 = axes[1, 0]\n",
    "cpu_data = speed_df[speed_df['Device'] == 'CPU']\n",
    "ax3.bar(['YOLO26n', 'YOLO11n'], \n",
    "        [cpu_data[cpu_data['Model']=='YOLO26n']['mean_ms'].values[0],\n",
    "         cpu_data[cpu_data['Model']=='YOLO11n']['mean_ms'].values[0]],\n",
    "        color=[colors['YOLO26n'], colors['YOLO11n']])\n",
    "ax3.set_ylabel('Inference Time (ms)')\n",
    "ax3.set_title(f'CPU Inference Speed\\n(YOLO26 is {cpu_speedup:.2f}x faster)')\n",
    "\n",
    "# 4. GPU Speed Comparison (if available)\n",
    "ax4 = axes[1, 1]\n",
    "if torch.cuda.is_available():\n",
    "    gpu_data = speed_df[speed_df['Device'] == 'GPU']\n",
    "    ax4.bar(['YOLO26n', 'YOLO11n'], \n",
    "            [gpu_data[gpu_data['Model']=='YOLO26n']['mean_ms'].values[0],\n",
    "             gpu_data[gpu_data['Model']=='YOLO11n']['mean_ms'].values[0]],\n",
    "            color=[colors['YOLO26n'], colors['YOLO11n']])\n",
    "    ax4.set_ylabel('Inference Time (ms)')\n",
    "    ax4.set_title(f'GPU Inference Speed\\n(YOLO26 is {gpu_speedup:.2f}x faster)')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'GPU not available', ha='center', va='center')\n",
    "    ax4.set_title('GPU Inference Speed')\n",
    "\n",
    "plt.suptitle('YOLO26 vs YOLO11 Benchmark on VisDrone', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('benchmark_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to: benchmark_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"YOLO26 vs YOLO11 Benchmark Summary - VisDrone Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. OVERALL ACCURACY:\")\n",
    "print(\"-\"*50)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "if VISDRONE_PATH:\n",
    "    print(\"\\n2. ACCURACY BY OBJECT SIZE (COCO):\")\n",
    "    print(\"-\"*50)\n",
    "    print(size_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n3. INFERENCE SPEED:\")\n",
    "print(\"-\"*50)\n",
    "print(speed_df[[\"Model\", \"Device\", \"mean_ms\", \"fps\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n4. KEY FINDINGS:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"   - CPU Speedup: YOLO26 is {cpu_speedup:.2f}x faster ({(cpu_speedup-1)*100:.1f}%)\")\n",
    "print(f\"   - Ultralytics Claim: 43% faster (1.43x) on CPU\")\n",
    "print(f\"   - Claim Verification: {'CONFIRMED' if cpu_speedup >= 1.43 else 'PARTIALLY CONFIRMED' if cpu_speedup >= 1.2 else 'NOT CONFIRMED'}\")\n",
    "\n",
    "if VISDRONE_PATH:\n",
    "    small_diff = size_df[size_df['Model']=='YOLO26n']['AP_small'].values[0] - size_df[size_df['Model']=='YOLO11n']['AP_small'].values[0]\n",
    "    print(f\"   - Small Object AP Improvement: {'+' if small_diff > 0 else ''}{small_diff:.4f}\")\n",
    "    print(f\"   - Small Object Claim: {'CONFIRMED' if small_diff > 0 else 'NOT CONFIRMED'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Upload Best Model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your HuggingFace token\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # Replace with your token\n",
    "HF_REPO = \"raimbekovm/yolo26-visdrone\"\n",
    "\n",
    "# Upload model\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "try:\n",
    "    # Upload YOLO26 model\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=YOLO26_WEIGHTS,\n",
    "        path_in_repo=\"yolo26n_visdrone.pt\",\n",
    "        repo_id=HF_REPO,\n",
    "        repo_type=\"space\",\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    print(f\"Successfully uploaded YOLO26n model to {HF_REPO}\")\n",
    "except Exception as e:\n",
    "    print(f\"Upload failed: {e}\")\n",
    "    print(\"You can manually upload the model to HuggingFace.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for later use\n",
    "metrics_df.to_csv(\"metrics_comparison.csv\", index=False)\n",
    "speed_df.to_csv(\"speed_benchmark.csv\", index=False)\n",
    "if VISDRONE_PATH:\n",
    "    size_df.to_csv(\"coco_evaluation.csv\", index=False)\n",
    "\n",
    "print(\"Results saved to CSV files.\")\n",
    "print(\"\\nBenchmark complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
