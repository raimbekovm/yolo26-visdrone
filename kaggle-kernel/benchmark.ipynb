{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO26 vs YOLO11 Benchmark on VisDrone\n",
    "\n",
    "**Goal:** Verify YOLO26 claims on drone imagery with small objects:\n",
    "1. **43% faster CPU inference**\n",
    "2. **Better small object detection** (ProgLoss + STAL)\n",
    "3. **NMS-free end-to-end inference**\n",
    "\n",
    "**Dataset:** VisDrone (~90% small objects <32px)\n",
    "\n",
    "**Author:** Murat Raimbekov | [GitHub](https://github.com/raimbekovm/yolo26-visdrone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install latest ultralytics from GitHub main branch (fixes end2end training bug)\n!pip install -q git+https://github.com/ultralytics/ultralytics.git pycocotools pandas matplotlib seaborn\nimport ultralytics\nprint(f\"Ultralytics version: {ultralytics.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport time\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom ultralytics import YOLO\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# CRITICAL FIX: Monkey-patch ultralytics to disable end2end on all model loads\n# This is necessary because YOLO26's end2end head causes:\n# \"RuntimeError: Inference tensors do not track version counter\"\n_original_yolo_init = YOLO.__init__\n\ndef _patched_yolo_init(self, *args, **kwargs):\n    _original_yolo_init(self, *args, **kwargs)\n    # Disable end2end after model is loaded\n    try:\n        if hasattr(self, 'model') and hasattr(self.model, 'model'):\n            head = self.model.model[-1]\n            if hasattr(head, 'end2end') and head.end2end:\n                head.end2end = False\n                print(f\"[PATCH] Disabled end2end for {type(head).__name__}\")\n    except Exception as e:\n        pass\n\nYOLO.__init__ = _patched_yolo_init\nprint(\"[PATCH] YOLO.__init__ patched to disable end2end\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'epochs': 50,  # Reduced for Kaggle time limits\n",
    "    'batch': 16,\n",
    "    'imgsz': 640,\n",
    "    'data': 'VisDrone.yaml',\n",
    "    'device': 0,\n",
    "    'workers': 4,\n",
    "    'project': 'runs',\n",
    "}\n",
    "\n",
    "# Speed benchmark config\n",
    "SPEED_CONFIG = {\n",
    "    'warmup': 50,\n",
    "    'runs': 200,\n",
    "}\n",
    "\n",
    "print(\"Config:\", CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train YOLO26n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"Training YOLO26n on VisDrone\")\nprint(\"=\"*60)\n\nyolo26 = YOLO('yolo26n.pt')\nyolo26.info()\n\nyolo26_results = yolo26.train(\n    data=CONFIG['data'],\n    epochs=CONFIG['epochs'],\n    batch=CONFIG['batch'],\n    imgsz=CONFIG['imgsz'],\n    device=CONFIG['device'],\n    workers=CONFIG['workers'],\n    project=CONFIG['project'],\n    name='yolo26n',\n    exist_ok=True,\n    plots=True,\n    verbose=True,\n)\n\nYOLO26_WEIGHTS = 'runs/yolo26n/weights/best.pt'\nprint(f\"\\nYOLO26n weights: {YOLO26_WEIGHTS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train YOLO11n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Training YOLO11n on VisDrone\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "yolo11 = YOLO('yolo11n.pt')\n",
    "yolo11.info()\n",
    "\n",
    "yolo11_results = yolo11.train(\n",
    "    data=CONFIG['data'],\n",
    "    epochs=CONFIG['epochs'],\n",
    "    batch=CONFIG['batch'],\n",
    "    imgsz=CONFIG['imgsz'],\n",
    "    device=CONFIG['device'],\n",
    "    workers=CONFIG['workers'],\n",
    "    project=CONFIG['project'],\n",
    "    name='yolo11n',\n",
    "    exist_ok=True,\n",
    "    plots=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "YOLO11_WEIGHTS = 'runs/yolo11n/weights/best.pt'\n",
    "print(f\"\\nYOLO11n weights: {YOLO11_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation - Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load trained models (monkey-patch will auto-disable end2end)\nprint(\"Loading trained models...\")\nyolo26_trained = YOLO(YOLO26_WEIGHTS)\nyolo11_trained = YOLO(YOLO11_WEIGHTS)\n\n# Validate\nprint(\"\\nValidating YOLO26n...\")\nval26 = yolo26_trained.val(data=CONFIG['data'], imgsz=CONFIG['imgsz'], device=CONFIG['device'])\n\nprint(\"\\nValidating YOLO11n...\")\nval11 = yolo11_trained.val(data=CONFIG['data'], imgsz=CONFIG['imgsz'], device=CONFIG['device'])\n\n# Extract metrics\nmetrics = pd.DataFrame({\n    'Model': ['YOLO26n', 'YOLO11n'],\n    'mAP50': [val26.box.map50, val11.box.map50],\n    'mAP50-95': [val26.box.map, val11.box.map],\n    'Precision': [val26.box.mp, val11.box.mp],\n    'Recall': [val26.box.mr, val11.box.mr],\n})\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"OVERALL METRICS\")\nprint(\"=\"*60)\nprint(metrics.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Speed Benchmark (CPU & GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def benchmark_speed(model, device, warmup=50, runs=200, imgsz=640):\n    \"\"\"Benchmark inference speed\"\"\"\n    dummy = np.random.randint(0, 255, (imgsz, imgsz, 3), dtype=np.uint8)\n    \n    # Warmup\n    for _ in range(warmup):\n        model.predict(dummy, device=device, verbose=False)\n    \n    if device != 'cpu' and torch.cuda.is_available():\n        torch.cuda.synchronize()\n    \n    # Benchmark\n    times = []\n    for _ in range(runs):\n        t0 = time.perf_counter()\n        model.predict(dummy, device=device, verbose=False)\n        if device != 'cpu' and torch.cuda.is_available():\n            torch.cuda.synchronize()\n        times.append((time.perf_counter() - t0) * 1000)\n    \n    return {\n        'mean': np.mean(times),\n        'std': np.std(times),\n        'min': np.min(times),\n        'max': np.max(times),\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_results = []\n",
    "\n",
    "# GPU Benchmark\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU SPEED BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nYOLO26n GPU...\")\n",
    "s26_gpu = benchmark_speed(yolo26_trained, device=0, **SPEED_CONFIG)\n",
    "speed_results.append({'Model': 'YOLO26n', 'Device': 'GPU', **s26_gpu})\n",
    "print(f\"  Mean: {s26_gpu['mean']:.2f} ms\")\n",
    "\n",
    "print(\"\\nYOLO11n GPU...\")\n",
    "s11_gpu = benchmark_speed(yolo11_trained, device=0, **SPEED_CONFIG)\n",
    "speed_results.append({'Model': 'YOLO11n', 'Device': 'GPU', **s11_gpu})\n",
    "print(f\"  Mean: {s11_gpu['mean']:.2f} ms\")\n",
    "\n",
    "gpu_speedup = s11_gpu['mean'] / s26_gpu['mean']\n",
    "print(f\"\\nGPU Speedup: YOLO26 is {gpu_speedup:.2f}x {'faster' if gpu_speedup > 1 else 'slower'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Benchmark (THE MAIN CLAIM: 43% faster)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CPU SPEED BENCHMARK (Main YOLO26 Claim: 43% faster)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nYOLO26n CPU... (this takes a while)\")\n",
    "s26_cpu = benchmark_speed(yolo26_trained, device='cpu', warmup=10, runs=50)\n",
    "speed_results.append({'Model': 'YOLO26n', 'Device': 'CPU', **s26_cpu})\n",
    "print(f\"  Mean: {s26_cpu['mean']:.2f} ms\")\n",
    "\n",
    "print(\"\\nYOLO11n CPU...\")\n",
    "s11_cpu = benchmark_speed(yolo11_trained, device='cpu', warmup=10, runs=50)\n",
    "speed_results.append({'Model': 'YOLO11n', 'Device': 'CPU', **s11_cpu})\n",
    "print(f\"  Mean: {s11_cpu['mean']:.2f} ms\")\n",
    "\n",
    "cpu_speedup = s11_cpu['mean'] / s26_cpu['mean']\n",
    "cpu_improvement = (1 - s26_cpu['mean'] / s11_cpu['mean']) * 100\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"CPU SPEEDUP RESULTS\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"YOLO26n: {s26_cpu['mean']:.2f} ms\")\n",
    "print(f\"YOLO11n: {s11_cpu['mean']:.2f} ms\")\n",
    "print(f\"Speedup: {cpu_speedup:.2f}x ({cpu_improvement:.1f}% faster)\")\n",
    "print(f\"\\nClaimed: 43% faster\")\n",
    "print(f\"Actual:  {cpu_improvement:.1f}% faster\")\n",
    "print(f\"Claim {'VERIFIED ✓' if cpu_improvement >= 40 else 'PARTIALLY VERIFIED' if cpu_improvement >= 20 else 'NOT VERIFIED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df = pd.DataFrame(speed_results)\n",
    "print(\"\\nSpeed Benchmark Results:\")\n",
    "print(speed_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. COCO Evaluation - mAP by Object Size\n",
    "\n",
    "Testing ProgLoss + STAL claim for small objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from PIL import Image\n",
    "\n",
    "CLASSES = ['pedestrian', 'people', 'bicycle', 'car', 'van',\n",
    "           'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor']\n",
    "\n",
    "# Find dataset path\n",
    "possible = [Path('datasets/VisDrone'), Path.home()/'datasets'/'VisDrone']\n",
    "DATA_PATH = next((p for p in possible if p.exists()), None)\n",
    "\n",
    "if DATA_PATH:\n",
    "    print(f\"Dataset: {DATA_PATH}\")\n",
    "else:\n",
    "    print(\"Dataset path not found, skipping COCO eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_coco_gt(data_path):\n    \"\"\"Create COCO ground truth from VisDrone labels\"\"\"\n    images_dir = data_path / 'VisDrone2019-DET-val' / 'images'\n    labels_dir = data_path / 'VisDrone2019-DET-val' / 'labels'\n    \n    coco = {'images': [], 'annotations': [], 'categories': [\n        {'id': i, 'name': n} for i, n in enumerate(CLASSES)\n    ]}\n    \n    ann_id = 0\n    for img_id, img_path in enumerate(sorted(images_dir.glob('*.jpg'))):\n        with Image.open(img_path) as im:\n            w, h = im.size\n        \n        coco['images'].append({'id': img_id, 'file_name': img_path.name, 'width': w, 'height': h})\n        \n        label_path = labels_dir / f\"{img_path.stem}.txt\"\n        if label_path.exists():\n            for line in open(label_path):\n                parts = line.strip().split()\n                if len(parts) >= 5:\n                    cls = int(parts[0])\n                    xc, yc, bw, bh = map(float, parts[1:5])\n                    xc, yc, bw, bh = xc*w, yc*h, bw*w, bh*h\n                    coco['annotations'].append({\n                        'id': ann_id, 'image_id': img_id, 'category_id': cls,\n                        'bbox': [xc-bw/2, yc-bh/2, bw, bh], 'area': bw*bh, 'iscrowd': 0\n                    })\n                    ann_id += 1\n    return coco\n\ndef get_predictions(model, data_path):\n    \"\"\"Get COCO format predictions\"\"\"\n    images_dir = data_path / 'VisDrone2019-DET-val' / 'images'\n    preds = []\n    \n    for img_id, img_path in enumerate(sorted(images_dir.glob('*.jpg'))):\n        results = model.predict(str(img_path), verbose=False)[0]\n        if results.boxes is not None:\n            for box, conf, cls in zip(results.boxes.xyxy.cpu().numpy(),\n                                       results.boxes.conf.cpu().numpy(),\n                                       results.boxes.cls.cpu().numpy()):\n                x1, y1, x2, y2 = box\n                preds.append({\n                    'image_id': img_id, 'category_id': int(cls),\n                    'bbox': [float(x1), float(y1), float(x2-x1), float(y2-y1)],\n                    'score': float(conf)\n                })\n    return preds"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_results = []\n",
    "\n",
    "if DATA_PATH:\n",
    "    print(\"Creating COCO ground truth...\")\n",
    "    gt_dict = create_coco_gt(DATA_PATH)\n",
    "    coco_gt = COCO()\n",
    "    coco_gt.dataset = gt_dict\n",
    "    coco_gt.createIndex()\n",
    "    print(f\"GT: {len(gt_dict['images'])} images, {len(gt_dict['annotations'])} annotations\")\n",
    "    \n",
    "    # YOLO26\n",
    "    print(\"\\nEvaluating YOLO26n...\")\n",
    "    preds26 = get_predictions(yolo26_trained, DATA_PATH)\n",
    "    coco_dt26 = coco_gt.loadRes(preds26)\n",
    "    eval26 = COCOeval(coco_gt, coco_dt26, 'bbox')\n",
    "    eval26.evaluate()\n",
    "    eval26.accumulate()\n",
    "    eval26.summarize()\n",
    "    \n",
    "    size_results.append({\n",
    "        'Model': 'YOLO26n',\n",
    "        'AP': eval26.stats[0],\n",
    "        'AP_small': eval26.stats[3],\n",
    "        'AP_medium': eval26.stats[4],\n",
    "        'AP_large': eval26.stats[5],\n",
    "    })\n",
    "    \n",
    "    # YOLO11\n",
    "    print(\"\\nEvaluating YOLO11n...\")\n",
    "    preds11 = get_predictions(yolo11_trained, DATA_PATH)\n",
    "    coco_dt11 = coco_gt.loadRes(preds11)\n",
    "    eval11 = COCOeval(coco_gt, coco_dt11, 'bbox')\n",
    "    eval11.evaluate()\n",
    "    eval11.accumulate()\n",
    "    eval11.summarize()\n",
    "    \n",
    "    size_results.append({\n",
    "        'Model': 'YOLO11n',\n",
    "        'AP': eval11.stats[0],\n",
    "        'AP_small': eval11.stats[3],\n",
    "        'AP_medium': eval11.stats[4],\n",
    "        'AP_large': eval11.stats[5],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if size_results:\n",
    "    size_df = pd.DataFrame(size_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"mAP BY OBJECT SIZE (ProgLoss + STAL Test)\")\n",
    "    print(\"=\"*60)\n",
    "    print(size_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate improvement\n",
    "    small_diff = size_df[size_df['Model']=='YOLO26n']['AP_small'].values[0] - \\\n",
    "                 size_df[size_df['Model']=='YOLO11n']['AP_small'].values[0]\n",
    "    \n",
    "    print(f\"\\nSmall Object AP Improvement: {'+' if small_diff > 0 else ''}{small_diff:.4f}\")\n",
    "    print(f\"ProgLoss + STAL Claim: {'VERIFIED ✓' if small_diff > 0 else 'NOT VERIFIED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "colors = {'YOLO26n': '#2ecc71', 'YOLO11n': '#3498db'}\n",
    "\n",
    "# 1. mAP Comparison\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(2)\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, metrics['mAP50'], w, label='mAP50', color='#3498db')\n",
    "ax.bar(x + w/2, metrics['mAP50-95'], w, label='mAP50-95', color='#2ecc71')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics['Model'])\n",
    "ax.set_ylabel('mAP')\n",
    "ax.set_title('Overall mAP Comparison')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 0.6)\n",
    "\n",
    "# 2. Speed Comparison\n",
    "ax = axes[0, 1]\n",
    "cpu_data = speed_df[speed_df['Device'] == 'CPU']\n",
    "bars = ax.bar(cpu_data['Model'], cpu_data['mean'], \n",
    "              color=[colors[m] for m in cpu_data['Model']], edgecolor='black')\n",
    "ax.set_ylabel('Inference Time (ms)')\n",
    "ax.set_title(f'CPU Speed (YOLO26 is {cpu_speedup:.1f}x faster)')\n",
    "for bar, val in zip(bars, cpu_data['mean']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "            f'{val:.1f}ms', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. mAP by Size\n",
    "ax = axes[1, 0]\n",
    "if size_results:\n",
    "    x = np.arange(3)\n",
    "    w = 0.35\n",
    "    ax.bar(x - w/2, [size_df[size_df['Model']=='YOLO26n'][c].values[0] for c in ['AP_small', 'AP_medium', 'AP_large']], \n",
    "           w, label='YOLO26n', color=colors['YOLO26n'])\n",
    "    ax.bar(x + w/2, [size_df[size_df['Model']=='YOLO11n'][c].values[0] for c in ['AP_small', 'AP_medium', 'AP_large']], \n",
    "           w, label='YOLO11n', color=colors['YOLO11n'])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['Small\\n(<32px)', 'Medium\\n(32-96px)', 'Large\\n(>96px)'])\n",
    "    ax.set_ylabel('AP')\n",
    "    ax.set_title('AP by Object Size (COCO Eval)')\n",
    "    ax.legend()\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'COCO eval not available', ha='center', va='center')\n",
    "\n",
    "# 4. GPU Speed\n",
    "ax = axes[1, 1]\n",
    "gpu_data = speed_df[speed_df['Device'] == 'GPU']\n",
    "bars = ax.bar(gpu_data['Model'], gpu_data['mean'], \n",
    "              color=[colors[m] for m in gpu_data['Model']], edgecolor='black')\n",
    "ax.set_ylabel('Inference Time (ms)')\n",
    "ax.set_title(f'GPU Speed (YOLO26 is {gpu_speedup:.1f}x faster)')\n",
    "for bar, val in zip(bars, gpu_data['mean']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "            f'{val:.1f}ms', ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('YOLO26 vs YOLO11 on VisDrone', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('benchmark_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"YOLO26 vs YOLO11 BENCHMARK SUMMARY - VisDrone Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. OVERALL ACCURACY\")\n",
    "print(\"-\"*50)\n",
    "print(metrics.to_string(index=False))\n",
    "\n",
    "print(\"\\n2. SPEED BENCHMARK\")\n",
    "print(\"-\"*50)\n",
    "print(speed_df[['Model', 'Device', 'mean']].to_string(index=False))\n",
    "\n",
    "if size_results:\n",
    "    print(\"\\n3. mAP BY OBJECT SIZE\")\n",
    "    print(\"-\"*50)\n",
    "    print(size_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n✓ CPU Speedup: {cpu_speedup:.2f}x ({cpu_improvement:.1f}% faster)\")\n",
    "print(f\"  Claimed: 43% faster\")\n",
    "print(f\"  Status: {'VERIFIED ✓' if cpu_improvement >= 40 else 'PARTIALLY VERIFIED' if cpu_improvement >= 20 else 'NOT VERIFIED'}\")\n",
    "\n",
    "print(f\"\\n✓ GPU Speedup: {gpu_speedup:.2f}x\")\n",
    "\n",
    "if size_results:\n",
    "    print(f\"\\n✓ Small Object Detection:\")\n",
    "    print(f\"  YOLO26 AP_small: {size_df[size_df['Model']=='YOLO26n']['AP_small'].values[0]:.4f}\")\n",
    "    print(f\"  YOLO11 AP_small: {size_df[size_df['Model']=='YOLO11n']['AP_small'].values[0]:.4f}\")\n",
    "    print(f\"  Improvement: {'+' if small_diff > 0 else ''}{small_diff:.4f}\")\n",
    "    print(f\"  ProgLoss+STAL: {'VERIFIED ✓' if small_diff > 0 else 'NOT VERIFIED'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "metrics.to_csv('metrics.csv', index=False)\n",
    "speed_df.to_csv('speed_benchmark.csv', index=False)\n",
    "if size_results:\n",
    "    size_df.to_csv('size_metrics.csv', index=False)\n",
    "\n",
    "print(\"Results saved to CSV files.\")\n",
    "print(\"\\nBenchmark complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}